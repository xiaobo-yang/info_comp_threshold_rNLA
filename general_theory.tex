\chapter{Theoretic Frameworks}
General complexity frameworks for general theoretic areas.


%---------------------------------------------------
%---------------------------------------------------
\section{Meta-complexity}
Some classical problems that are usually reduced to.
%---------------------------------------------------
\subsection{Communication complexity}
\subsubsection{Gap-Hamming-Distance problem}
\begin{prob}[Gap-Hamming\cite{mmmw21}]
\label{ghd}
     Let Alice and Bob be communicating parties who hold vectors $s\in\{\pm1\}^n$ and $t\in\{\pm1\}^n$, respectively. The Gap-Hamming problem asks Alice and Bob to return
     \begin{align*}
         1~if~\langle s,t \rangle\ge \sqrt{n}~~~~~~and~~~~~~-1~if~\langle s,t \rangle\le -\sqrt{n}.
     \end{align*}
     Denote it by $GHD_{n}(x,y)$.
\end{prob}



In general, we need to compute the function $f(x,y)$ for $x\in \mc X, y\in\mc Y$, where $\mc X$ and $\mc Y$ are separate sources and we cannot simultaneously obtain both $x$ and $y$. Instead, we repeatedly communicate information from $x$ to $y$ or from $y$ to $x$, accumulating intermediate information during this process. Finally, we use this accumulative intermediate information to compute $f(x,y)$. The goal is to minimize the number of times of communication or the total amount of communication bits.

To formalize this communication process, a binary tree structure called a "protocol tree" $T$(Fig.\ref{proto_tree}) is used. Each possible value of $f(x,y)$ corresponds to a leaf node $L$ in $T$. The communication complexity is measured by the height $H(T)$ of the tree $T$. Notably, for any node $N$ in $T$, the set $R(N):={(x,y):(x,y)\text{ can reach node $N$}}$ always forms a square, i.e., $R(N)=A\times B$ for some $A\subseteq\mc X$ and $B\subseteq\mc Y$. As a result, the leaf nodes form a rectangular partition of $\mc X\times\mc Y$.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{protocol_tree.png}
  \caption{Protocol tree\cite{kushilevitz1997communication}}
  \label{proto_tree}
\end{figure}

By using the minimum number of square partitions $M_f$ of the function $f(x,y)$, we can derive a lower bound on the communication complexity:

\[
H(T)\ge \log_2 M_f.
\]


\begin{thm}[\cite{cr12}\cite{vidick2012concentration}\cite{sherstov2012communication}]
\label{ghd_lower_bound}
    Any randomized protocol solving problem \ref{ghd} $GHD_n$ with probability $2/3$ needs at least $\Omega(n)$ bits of communication.
\end{thm}

\pfsk{\ref{ghd_lower_bound}}{
The original proof is in \cite{cr12} and \cite{vidick2012concentration} make a simpler one. Here we use proof in \cite{sherstov2012communication}, which is simplist among them.

First, use \textbf{Yao's principle}(theorem \ref{yaoprin}) to reduce the randomness of algorithm to randomness of a distribution $\mu$ of input $(x,y)$. Formally, consider all those deterministic protocol tree $T(x,y)$ such that $\mb P_{(x,y)\sim\mu}[T(x,y)\neq f(x,y)]\le\epsilon$. Denote the minimum complexity(tree height) of such deterministic $T$ by $R_{\mu,\epsilon}(f)$, and denote minimum expectation complexity of random tree with error $\epsilon$ by $C_\epsilon(f)$, we have $C_\epsilon(f)\gtrsim R_{\mu,\epsilon}(f)$. See \cite{yao83} lemma 2 in sect. 3.3 for details.

Then, it suffices to find a hard distribution $\mu$ of $(x,y)$ and lower bound of $R_{\mu,\epsilon}(f)$.
This is done by a trick called \textbf{$\epsilon$-corruption bound}:
\begin{lem}[Yao's corruption bound\cite{sherstov2012communication}]
If $\exists \varepsilon, \delta \in(0,1)$ such that $\mu$ satisfies
$$\mu\left(R \cap f^{-1}(+1)\right)>\varepsilon \mu\left(R \cap f^{-1}(-1)\right)$$
for every rectangle $R \subseteq X \times Y$ with $\mu(R)>\delta$, 
 then $R_{\mu,\xi}(f)\ge\log_2\bbr{\frac{1}{\delta}\left(\mu\left(f^{-1}(-1)\right)-\frac{\xi}{\varepsilon}\right)}$ for small $\xi>0$.
\end{lem}
\noindent Intuitively, the misclassification error in any large rectangle is not so large. Therefore, when total error is a given constant, we can lower bound the number of rectangle partition and thus lower bound the tree height. See \cite{yao83} lemma 3 in sect. 3.3 for originality and details. Usually, people try to directly prove corruption bound for uniformly distribution.

Finally, in our setting, the Gap-Hamming distance problem($GHD_n$) is further reduce to Gap-orthogonality problem($ORT_n$, problem \ref{ort_prob}) with $R_{1/3}(GHD_n)=\Omega(R_{1/3}(ORT_n))$, where $f(x,y)=(-1)\mf 1_{|\langle x,y \rangle|\le\sqrt{n}}+(+1)\mf 1_{|\langle x,y \rangle|\ge2\sqrt{n}}$. 
Note that $f^{-1}(+1)=\{(x,y)||\langle x,y \rangle|\ge2\sqrt{n}\}$, thus the corruption bound intuitively(not so rigorously) means for rectangle $R$ with large probability, we have
\begin{equation}
    \label{anti_concen_ghd}
    \exists c,C>0,~~\mb P_{(x,y)\sim Unif[R]}[|\langle x,y \rangle|>C]\ge c,
\end{equation}
which is essentially a \textbf{anti-concentration analysis}. Choosing uniform distribution is enough, it is proved by \textbf{probabilistic method} with Talagrand's inequalities(corollary \ref{tal_ineq_proj}) and a interesting combinatorial accounting trick:
\begin{lem}[Corruption bound\cite{sherstov2012communication}]
\label{cor_bound}
Let $\mu$ be uniform distribution on $\{\pm1\}^n\times\{\pm1\}^n$ and $R=A\times B$ be a rectangle such that $\mu(R\cap f^{-1}(+1))\le \epsilon\mu(R)$ then $\mu(R)=e^{-\Omega(n)}.$ is small. In other word, if $\mu(R)$ is large, we have corruption bound.
\end{lem}
\pfskm{lemma}{\ref{cor_bound}}{
The anti-concentration inequality is reduced to that, by corollary \ref{tal_ineq_proj} we can find a set $I$ of near-orthogonal vectors of $x\in A$ in rectangle $R=A\times B$ with large probability, and then by some linear algebra and Hoffman-Wielandt inequality, for most of $y\in B$, there must exists a $x'\in I$ such that $|\langle x',y\rangle|$ is large. 
}
\noindent Use the lemma, we compute $C_{1/3}(GHD_n)=\Omega(R_{1/3}(ORT_n))=\Omega(n)$.
}

\begin{rmk}
For general communication problem, the analysis of average-case complexity usually follows the roadmap above:
\begin{enumerate}
    \item Use Yao's principle to reduce the problem with randomized algorithm into a randomized input with deterministic algorithm.
    \item Find the math structures of complexity in setting of deterministic algorithm(In communication problem, it's height of a binary tree whose leaf nodes are rectangle partition).
    \item Abstract these structures into pure math language, and then use fundamental math tool to work it out(In this communication function $f=GHD_n$, the structure is an anti-concentration analysis and tools are some probabilistic methods).
\end{enumerate}
\end{rmk}

\begin{rmk}
Another proof\cite{vidick2012concentration} use a different description of the anti-concentration(theorem \ref{large_overlap}) in corruption bound \eqref{anti_concen_ghd}, where they consider general vectors $x,y\in\R^n$($\mc X,\mc Y, R$ may be continuous).
\end{rmk}
%---------------------------------------------------
\subsubsection{Approximate-Orthogonality problem}
\begin{prob}[\cite{chakrabarti2012information}\cite{woodruff2022optimal}]
\label{ort_prob}
Let Alice and Bob be communicating parties who hold vectors $s\in\{\pm1\}^n$ and $t\in\{\pm1\}^n$, respectively. The Approximate-Orthogonality problem asks Alice and Bob to return
\begin{align*}
    1~if~|\langle s,t \rangle|\le b\sqrt{n}~~~~~~and~~~~~~-1~~~otherwise.
\end{align*}
Denote it by $ORT_{b,n}(x,y)$.
\end{prob}

\begin{thm}[\cite{chakrabarti2012information}\cite{woodruff2022optimal}]
\label{ort_complexity}
Any randomized protocol solving problem \ref{ort_prob} $ORT_{b,n}$ with probability at least $\Phi(2.01\max\{66,b\})$($b>1/5$) needs at least $\Omega(n)$ bits of communication. Where $\Phi(x)$
is $c.d.f.$ of $\mc N(0,1)$.
\end{thm}
\pfsk{\ref{ort_complexity}}{
    \xb{based on anti-concentration lemma \ref{ort_anti_concen}}
}
%---------------------------------------------------
\subsubsection{Gap-Equality problem}
\begin{prob}[\cite{woodruff2022optimal}]
\label{eq_prob}
Let Alice and Bob be communicating parties who hold vectors $s\in\{0,1\}^n$ and $t\in\{0,1\}^n$ with either $s=t$ or $\|s-t\|_2^2=n/2$, respectively. The Gap-Equality problem asks Alice and Bob to return
\begin{align*}
    1~if~s=t~~~~~~and~~~~~~-1~~~otherwise.
\end{align*}
Denote it by $EQ_{n}(x,y)$.
\end{prob}
\begin{thm}
\label{eq_complexity}
Any deterministic protocol solving problem \ref{eq_prob} $EQ_n$ needs at least $\Omega(n)$ bits of communication.
\end{thm}

%---------------------------------------------------
\subsubsection{Augmented Indexing problem}
\begin{prob}[\cite{woodruff2022optimal}]
\label{ind_prob}
Given set $\mc U$ and an elements $\perp\notin\mc U$.
Let Alice and Bob be communicating parties who hold vectors $s\in\mc U^n$ and $t\in\{\mc U\cup\{\perp\}\}^n$
with either $s_k=t_k,~k<i$, $t_i\in\mc U$ and $y_{i+1}=\cdots=y_n=\perp$ for some unique $i$.
The Gap-Equality problem asks Alice and Bob to return
\begin{align*}
    1~if~s_i=t_i~~~~~~and~~~~~~-1~~~otherwise.
\end{align*}
Denote it by $IND_{n,\mc U}(x,y)$.
\end{prob}
\begin{thm}
\label{ind_complexity}
Any one-way(only one people can send message to the other people) randomized protocol solving problem \ref{ind_prob} $IND_{n,\mc U}(x,y)$ with error $\delta\le\frac{1}{4|\mc U|}$ needs at least $n\log|\mc U|/2$ bits of communication.
\end{thm}

%---------------------------------------------------
\subsection{Spiked wishart matrix testing}
\begin{prob}[Spiked wishart matrix\cite{mmmw21}\cite{pwbm18}]
\label{spiked_wishart}
Let $n=1/\epsilon$ and let $z\in\R^n$ be a uniformly random unit vector. Let $N\in\R^{n\times m}$ be $m$ $i.i.d$ Gaussian vector drawn from an $n$-dimensional $\mc N(0,C)$, where $C=I$ or $I-zz^T$. Use $N$ to identify $C$.
\end{prob}

\begin{thm}[\cite{mmmw21}]
\label{spiked_whishart_lower_bound}
To solve problem \ref{spiked_wishart} with probability at least $2/3$, we need at least $m=\Omega\bbr{\frac{1}{\epsilon}}$.
\end{thm}
\pfsk{\ref{spiked_whishart_lower_bound}}{First, let $P$ denote the distribution of $N$ under null hypothesis $C=I$, and $Q$ denote the distribution of $N$ under alternative hypothesis $C=I-zz^T$. Any testing statistics $\phi$ outputs $1$ for $Q$ and $0$ for $P$. It suffices to bound the total variation distance $d_{TV}(P,Q)$, since its control the summation of Type-1 and Type-2 errors
\begin{align*}
    \min_\phi\bbwr{P(\phi=1)+Q(\phi=0)}=1-d_{TV}(P,Q).
\end{align*}

Then, by Pinsker's inequality, we have $d_{TV}(Q,P)\le\sqrt{\frac{1}{2}D_{KL}(Q||P)}\le\sqrt{\frac{1}{2}D_{\chi^2}(Q||P)}.$
Therefore it suffices to upper bound $D_{\chi^2}(Q||P)=\int_{N}\bbr{\frac{Q(N)}{P(N)}}^2P(N)dN - 1$. By theorem \ref{spiked_gaussian_chi_dist} with $\beta=-1$ and spiked prior $z$, we have
\begin{align*}
    D_{\chi^2}(Q||P) = \mb E_{v,v'}\bbfr{(1-\langle v,v' \rangle^2)^{-m/2}}-1,
\end{align*}
where $v,v\stackrel{\mathrm{dist}}{=}z$ are uniformly random unit vectors. Therefore, by classical results of random unit vectors, $p.d.f.$ of $x:=\langle v, v'\rangle$ is $p(x)=\frac{\Gamma(n-1)}{2\Gamma((n-1)/2)^2}\bbr{\frac{1-x^2}{4}}^{(n-1)/2-1}$.

Finally, direct calculation yields, if $m=O\bbr{1/\epsilon}$, $\mb E_{v,v'}\bbfr{(1-\langle v,v' \rangle^2)^{-m/2}}<6/5$ and thus $d_{TV}(Q,P)<1/3$. Therefore, one of Type-1 and Type-2 errors must be larger than $1/3$.
}

\begin{prob}[\cite{jpwz21}]
    \label{hard_psd_testing}
    Given $\delta\in(0,1/2)$, set $n=\log(1/\delta)$. Independently take $g\sim\mc N(0,I_n)$ and $G\in\R^{n\times n}$ with $i.i.d.$ $\mc N(0,1)$ entries. Let $W=G+G^T$. Consider two distributions:
    \begin{itemize}
        \item Distribution $P$: $C\log^{3/2}(1/\delta)\cdot\frac{1}{\|g\|_2^2}gg^T + W + 6\sqrt{\log(1/\delta)}I_n$ for some fixed constant $C>0$.
        \item Distribution $Q$: $W + 6\sqrt{\log(1/\delta)}I_n$.
    \end{itemize}
    Use \textbf{non-adaptive} queries $q_1,...q_m$ and oracles $Aq_1,...,Aq_m$ to distinguish $P$ and $Q$. In other word, take a matrix $Q\in\R^{n\times m}$ as query and $AQ$ as oracle.
\end{prob}
\begin{thm}[\cite{jpwz21}]
    \label{hard_psd_testing_bound}
    Any randomized algorithm solving problem \ref{hard_psd_testing} with probability $1-\delta$ need at least $m=\Omega\bbr{\frac{\log(1/\delta)}{\log\log(1/\delta)}}$ queries. 
\end{thm}
\pfsk{\ref{hard_psd_testing_bound}}{
Similar to the proof of theorem \ref{spiked_whishart_lower_bound}, it is naturally to consider bounding the total variation $d_{TV}(P,Q)$, and by rotational invariance we WLOG assume $Q=E_m$, the first $m$ standard basis vectors. 

First, denote by $P',Q'$ the distribution of $BQ$($B\sim P$ or $Q$ respectively). Let $L_{P'},L_{Q'}\in\R^{l}$ be vectorization of matrices from $P',Q'$(remove the redundant variable, since $B$ is symmetric), where $l=n+(n-1)+\cdots (n-m+1)$. Observe that conditioned on a realization $g$ we have
\begin{align*}
    d_{KL}(P',Q'|g)&\le d_{KL}(L_{P'},L_{Q'}|g)
    \le \|\mb EL_{P'} - \mb EL_{Q'}\|_2^2=\sum_{i=1}^m\left\|C\log^{3/2}(1/\delta)\frac{gg^T}{\|g\|_2^2}e_i\right\|_2^2=C^2\log^{3}(1/\delta)\frac{\|g^T Q\|_2^2}{\|g\|_2^2}.
\end{align*}
The first inequality use data processing inequality(theorem \ref{data_process}) and the second inequality uses theorem \ref{gauss_kl}.

Then, find a typical event $\mc E$ with positive probability. We take $\mc E=\bbwr{\frac{\|g^T Q\|_2^2}{\|g\|_2^2}\le\frac{1}{50C^2n^3}}$ and show that $\mb P(\mc E)\ge 10\delta$ if we only take $m=O\bbr{\frac{\log(1/\delta)}{\log\log(1/\delta)}}$ queries. Indeed, assume $m\le n/2$
$$
\mb P(\mc E)\ge \mb P(\|g\|_2^2\ge n/2|\|g^T Q\|_2^2\ge 1/(100C^2n^2))\cdot\mb P(\|g^T Q\|_2^2\ge 1/(100C^2n^2))
= \Omega(1)\cdot\Omega((\frac{1}{n\sqrt{m}})^m)=\Omega(e^{-\frac{m}{2}\log(n^2m)}).
$$
Therefore, when $m=O\bbr{\frac{\log(1/\delta)}{\log\log(1/\delta)}}$, $\mb P(\mc E)\ge 10\delta$.

Finally, conditioned realization $g\in\mc E$, we have the total variation $d_{TV}(P',Q'|g)\le\sqrt{d_{TV}(P',Q'|g)/2}\le 1/3$ by Pinsker's inequality. This means $\mb P[\text{the algorithms make mistake}|g]\ge 1/3$ under $P'$ or $Q'$. Since $\mb P(\mc E)\ge 10\delta$, we have $\mb P[\text{the algorithms make mistake}]\ge \delta$ under $P$ or $Q$ if we only take $m=O\bbr{\frac{\log(1/\delta)}{\log\log(1/\delta)}}$ queries.
}
\begin{rmk}
This bound is independent of error $\epsilon$. Informally, this is because there is a sharp phase transition due to its mathematical structure. Similar structures can be found in proof of theorem \ref{tr_est_ada_schatten_p}, provided by Gap-Equality problem \ref{eq_prob}'s sharp contrast.
\end{rmk}
\xb{consider directly bound $d_{TV}(P,Q)\le 1-2\delta$ instead?}


%---------------------------------------------------
%---------------------------------------------------
%---------------------------------------------------
\section{Informational-computational gap}
The informational-computational gap refers to the phenomenon where certain statistical problems are informationally solvable, yet lack efficient (polynomial-time) algorithms in some regimes.

Three major research direction: \xb{need to add citations below, see intro of \cite{bandeira2022franzparisi} and appendix A of COLT verision of \cite{brennan2020reducibility}}
\begin{enumerate}
    \item Reduction form different average-case problems. reduced them to some classical forms, e.g. planted cliques.
    \item Prove the failure of the problems under some general classes of algorithms, e.g. sum of squares, low degree polynomial, statistical query model etc.
    \item Analyze the phenomena appeared in hard regime, e.g. overlap gap property, low degree likelihood ratio etc.
\end{enumerate}
%---------------------------------------------------
%---------------------------------------------------
\subsection{Average-case reduction}
Three principles\cite{brennan2020reducibility}
\begin{enumerate}
    \item Correctly map the distribution of noise and signal
    \item Lower bound from reduction must be tight(achieved by best known efficien algorithm)
    \item Tackle the different amount of parameters of the problem to reduce, e.g. biclustering problem has 2 parameters while planted clique has only one.
\end{enumerate}
%---------------------------------------------------
\subsubsection{An example of reduction: reduce biclustering to planted cliques}
This section is mainly from \cite{avg_redu_1} and \cite{avg_redu_2}.

%---------------------------------------------------
\subsubsection{Problems}

\begin{prob}[Biclustering]
\label{bc_prob}
Given a random matrix $X\in\rn n $, a biclustering problem with parameter $n,k,\lambda$ is the following hypothesis testing problem
\begin{itemize}
    \item $H_0$: $X_{ij}\stackrel{i.i.d.}{\sim}\mc N(0,1)$
    \item $H_1$: there exists $S,T\stackrel{i.i.d.}{\sim}\binom{[n]}{k}$, $X_{ij}\stackrel{i.i.d.}{\sim}
    \begin{cases}
        \mc N(\lambda,1) & \text{if $i\in S,j\in T$,}\\
      \mc N(0,1) & \text{otherwise.}
    \end{cases}$ 
\end{itemize}
Denote it by $BC(n,k,\lambda)$.
\end{prob}

\begin{prob}[Planted cliques]
\label{pc_prob}
Given a random graph $G=(V,E)$, a planted cliques problem with parameter $n,k$ is the following hypothesis testing problem
\begin{itemize}
    \item $H_0$: $E_{ij}\stackrel{i.i.d.}{\sim}Ber(1/2)$ for $i>j$,
    \item $H_1$: there exists $U\sim\binom{[n]}{k}$, $E_{ij}=
    \begin{cases}
      1 & \text{if $i,j\in U$,}\\
      Ber(1/2) & \text{otherwise.}
    \end{cases}$ 
\end{itemize}
Denote it by $PC(n,k)$.
\end{prob}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{pc.jpg}
        \caption{$PC(n,k)$, picture from \cite{info_comp_alg_equiv}}
        \label{pc_threshold}
    \end{subfigure}
    \hspace{3cm}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{bc.jpg}
        \caption{$BC(n,k,\lambda)$, picture from \cite{avg_redu_1}}
        \label{bc_threshold}
    \end{subfigure}
    \caption{Phase transition of informational and computational thresholds}
    \label{pc_bc_threshold}
\end{figure}
The phase transition of informational thresholds of both problems can be proved. However, we currently only have evidence for computational thresholds, lacking definitive proof.
\begin{itemize}
    \item For $PC(n,k)$ with $k=n^{\alpha}$($\alpha>1/2$), the statistics $T(G)=\mf{1}_{\text{max degree}\ge n/2+\sqrt{Cn\log n}}$ for sufficiently large constant $C$ succeeds to detect the hypothesis with polynomial time.
    This is because
    \begin{itemize}
        \item Under $H_0$, $\mb P(T=1)\le n\mb P(Ber(n,1/2)\ge n/2+\sqrt{Cn\log n})\le n\cdot n^{-2}\rightarrow 0$;
        \item Under $H_1$, 
        \begin{align*}
            \mb P(T=0)&\le \mb P(\text{for }i\text{ in the planted clique }deg(i)\le n/2+\sqrt{Cn\log n})\\
            &=\mb P(Ber(n-k,1/2)\le n/2-k+\sqrt{Cn\log n})^k\le \mb P\left(|\sum_{i=1}^{n-k}X_i-\mb EX_i|\ge k/2+\sqrt{Cn\log n}\right)^k\\
            &\le 2^ke^{-c\frac{k^2}{n}\cdot k} \rightarrow 0.
        \end{align*}
    \end{itemize}
    
    But there is no efficient algorithm for $\alpha\le 1/2$.

    \item For $BC(n,k,\lambda)$\cite{ma2015computational} with $k=n^\alpha$ and $\lambda=n^{-\beta}$,we first show some poly-time statistics in easy regime
    \begin{itemize}
        \item if $\beta< 0$, the maximum statistics $T_{max}(X)=\max_{i,j}X_{i,j}=\begin{cases}
            O(\sqrt{\log n}) & \text{under } H_0 \\
            \Omega(\lambda) & \text{under } H_1.
        \end{cases}$ 
        Thus it succeeds to detect the hypothesis with polynomial time.
        \item if $\beta> 0$, the average statistics $T_{avg}(X)=\frac{1}{n}\sum_{i,j}X_{i,j}=\begin{cases}
            \mc N(0,1) & \text{under } H_0 \\
            \mc N(\frac{k^2\lambda}{n},1) & \text{under } H_1.
        \end{cases}$
        Thus it succeeds to detect the hypothesis with polynomial time when $\beta<2\alpha - 1$.
    \end{itemize}
    But there is no efficient algorithm in regime among impossible regime and easy regime, which is conjectured to be hard.
    Though there does have some statistics solve the problem in this regime, they are not poly-time. 
    
    For example, the searching statistics $T_{search}=\frac{1}{k}\max_{|S|=|T|=k}\sum_{i\in S,j\in T}X_{ij}\begin{cases}
        =O(\sqrt{k\log n}) & \text{under } H_0 \\
        \ge \frac{k\lambda}{\sqrt{\log n}} ~w.h.p.~& \text{under } H_1
    \end{cases}$, where the first case is because $T_{search}\approx\max\text{ of }\binom{n}{k}^2~\mc N(0,1)$ 
    and the second case is because $T\ge \frac{1}{k}\sum_{i,j:X_{i,j}\sim\mc N(\lambda,1)}X_{i,j}\sim \mc N(k\lambda,1)$. 
    
    Therefore $\lambda\gg\frac{\log n}{\sqrt{k}}$ or $\beta<\alpha/2$ can solve the problem. Later we will see this is actually informational boundary.
\end{itemize}

\begin{thm}
    \label{pc_info_threshold}
    For $PC(n,k)$, the informational boundary is $2\log n$.
\end{thm}
\pfsk{\ref{pc_info_threshold}}{
    The proof is based on second moment method, see \cite{planted_clique_proof} for details.
}


\begin{thm}
    \label{bc_info_threshold}
    For $BC(n,k,\lambda)$, the informational boundary are the line segments shown in figure \ref{bc_threshold}.
\end{thm}
\pfsk{\ref{bc_info_threshold}}{
    The proof is to prove $d_{TV}(H_0,H_1)\rightarrow 0$, see \cite{brennan2019reducibility} theorem 40 for details.
}

\begin{conj}
    The phase transition of computational thresholds are in figure \ref{pc_bc_threshold}.
\end{conj}

\vspace{10pt}

%---------------------------------------------------
\subsubsection{Reduction}

We will show how to derive computational boundary between easy and hard regime of $BC(n,k,\lambda)$, assuming hardness result of $PC(n,k)$. But first we need the following prelinminary knowledge.
\begin{defn}[Worst-case reduction]
Let $P, P'$ be two worst-case problems. We say that $P$ reduces to $P'$, denoted by $P \leq_p P'$ if there is a polynomial-time computable algorithm $\mc A$ such that
$$
\mc A(P) = P',
$$
which means operating on the problem instance of $P$ to construct a problem instance of $P'$.
We say that $P$ and $P'$ have equivalent complexity if $P \leq_p P'$ and $P' \leq_p P$.
\end{defn}
\textbf{Example.} Reduce 3-SAT to independnent set problem, where the constructed graph $G$ has an independent set of size $k$ iff SAT formula $\Phi$ has a satisfying assignment. 

\textbf{Remark.} Intuitively, we may think hardness as \textbf{"amount of algorithms to solve the problem"}. In that way, $\mc A(P) = P'$ means all algorithms that can solve $P'$, can also solve $P$ combining with $\mc A$. Therefore, algorithms for $P$ is no fewer than $P'$ and $P$ is no harder than $P'$.

Compared to worst-case reduction, average-case reduction is significantly more complex because it involves not only mapping between structurally similar objects, but also mapping between distributions such that their statistical distances are small.

\begin{lem}[Total variation]
    \label{tv_def}
    Given two distribution $\mu$ and $\nu$, the total variation distance $d_{TV}(\mu,\nu)$ has the following equivalent definitions
    \begin{align*}
        d_{TV}(\mu,\nu) &= \frac{1}{2}\|\mu-\nu\|_1 
                        = \frac{1}{2}\int |f(x)-g(x)|dx ~~\text{if they have density $f,g$}\\
                        &= \sup_{E}|\mu(E) - \nu(E)| 
                        = \inf_{(X,Y),X\sim\mu,Y\sim\nu}\mb{P}(X\neq Y)\\
                        &=1- \min_{\phi:\phi\text{ is a testing statistics}}\{\mu(\phi\text{ indicates }\nu)+\nu(\phi\text{ indicates }\mu)\}.
    \end{align*}
\end{lem}

\begin{lem}
    \label{tv_coup}
    If random variables $X,Y$ satisfy that $d_{TV}(X,Y)\le\delta$, then there exists a coupling distribution $\mc P$ such that $\mb P_{(X,Y)\sim\mc P}(X=Y)\ge 1-\delta$.
\end{lem}

\begin{defn}[Reduction]
    Let $P, P'$ be two average-case problems. The reduction with error $\epsilon$ from $P$ to $P'$ is an polynomial-time computable algorithm $\mc A$ such that
    \[
        d_{TV}(\mc A(P), P')\le\epsilon.
    \]
\end{defn}

\begin{lem}[Properties of reduction]
    \label{property_redu}
    \begin{itemize}
        \item  Let algorithm $\mc A$ be reduction with error $\epsilon$ from $P$ to $P'$. If algorithm $\Phi$ solves $P'$ with probability $\delta$, then $\Phi\circ\mc A$ solves $P$ with probability $\delta+\epsilon$.
        \item  Let algorithm $\mc A_1$ be reduction with error $\epsilon_1$ from $P$ to $P_1$, and algorithm $\mc A_2$ be reduction with error $\epsilon_2$ from $P_1$ to $P_2$. Then $\mc A:=\mc A_2\circ \mc A_1$ is reduction with error $\epsilon_1+\epsilon_2$ from $P$ to $P_2$.
    \end{itemize}
\end{lem}
\pfm{lemma}{\ref{property_redu}}{
    \begin{enumerate}
        \item $\mb P_{X\sim P}(\phi\circ A \text{ solves } X)\le d_{TV} + \mb P_{Y\sim P'}(\phi \text{ solves } Y)\le \epsilon + \delta$.
        \item $d_{TV}(\mc A_2\circ\mc A_1(P),P_2)\le d_{TV}(\mc A_2\circ\mc A_1(P),\mc A_2(P_1)) + d_{TV}(\mc A_2(P_1),P_2)\le \epsilon_1+\epsilon_2$ by data processing inequality.
    \end{enumerate}
}

Now, we try to reduce problem $P=PC(n,k)$ to problem $P'=BC(n,k,\lambda)$. 
It's natural to map adjacency matrix of the random graph in $P$ to random matrix in $P'$. 
However, it's not trivial since the randomness are not the same, and the adjacency matrix is symmetric and without diagonal entries.

To solve these problems, the reduction is constructed by the following steps:
\begin{enumerate}
    \item Map Bernoulli variables to Gaussian variables.
    
        To find a mapping $\Phi$ such that $\Phi(1)=\mc N(\lambda,1)$ and $\Phi(Ber(1/2))=\mc N(0,1)$. 
        Denote p.d.f of $\mc N(\lambda,1),\mc N(0,1)$ by $P,Q$ respectively. Then p.d.f. of $\Phi(0)$ should be $2Q-P$.

        However, this density may not be positive all the time. Therefore we take truncation p.d.f.\ such that $\Phi(0)\sim\rho(x):=\frac{1}{Z}(2Q(x)-P(x))\mf{1}_{2Q-P>0}$ instead.
        Though it's not exact mapping we want, but fortunately we have $\Phi(Ber(1/2))\approx Q$.
        \begin{lem}
            \label{ber_to_gau_trunc_approx}
            If $\lambda\le O(\frac{1}{\sqrt{\log n}})$, then $d_{TV}(\Phi(Ber(1/2)),Q)=o(n^{-3})$.
        \end{lem}
        \pfskm{lemma}{\ref{ber_to_gau_trunc_approx}}{
            First note that the support is $(-\infty,\frac{\lambda}{2}+\frac{1}{\lambda}\log 2)$, then direct calculation yields $Z=1+o(n^{-3})$.
            Therefore $d_{TV}=\frac{1}{2}\left(\int|\frac{v(x)+P(x)}{2}-Q(x)|\mf 1_{2Q-P> 0}dx + \int|\frac{P(x)}{2}-Q(x)|\mf 1_{2Q-P< 0}dx \right)= o(n^{-3})$.
        }
        \begin{lem}
            \label{tv_tensor}
            If $X_1\indep X_2$ and $Y_1\indep Y_2$, then $d_{TV}((X_1,X_2),(Y_1,Y_2))\le d_{TV}(X_1,Y_1)+d_{TV}(X_2,Y_2)$.
        \end{lem}
        Now we construct matrix $W\in\rn n$, such that $W_{ij}=W_{ji}=\Phi(E_{ij})$ for $i\neq j$ otherwise $0$.
        By tensorization lemma \ref{tv_tensor}, the total variation distance between upper triangle of $W$ and $X$ is
        \[d_{TV}(W_{upper},X_{upper})\le o(n^2\cdot n^{-3})=o(n^{-1}).\]

    \item Clone Gaussian variables to symmetrize.
        
        There is an algorithm to clone independent copies of any gaussian variable.
        \begin{thm}
            \label{gauss_clone} 
            Let $X\sim\mc N(\mu,\sigma^2)$ as input, we can output $\mc N(\frac{\mu}{\sqrt{2}},\sigma^2)^{\otimes 2}$.
        \end{thm}
        \pfsk{\ref{gauss_clone}}{
            Let $Z\indep X$ with $Z\sim\mc N(0,\sigma^2)$, 
            then the joint distribution of $\frac{1}{\sqrt{2}}\begin{bmatrix}
                1 & 1\\
                1 & -1
            \end{bmatrix}\begin{pmatrix}
                X \\
                Z
            \end{pmatrix}$ is what we want.
        }
        Now we take an antisymmetric matrix $A$ with i.i.d. $\mc N(0, 1)$ random variables below
        its main diagonal and set $W\leftarrow \frac{1}{\sqrt{2}}(W+Z)$. Then we have the total variation distance between off-diagonal entries of $W$ and $X$ is 
        $$d_{TV}(W_{off},X_{off})=o(n^{-1})$$.

    \item Add diagonal randomness.
        
        Fill the diagonal entries of $W$ with $i.i.d.$ $\mc N(0,1)$ and permutation the columns randomly. The following techical lemma guarantee the total variation may not be too large.
        \begin{lem}[lemma 8 in \cite{brennan2019reducibility}]
            \label{off_diag_permut}
            Given distribution $P$ and $Q$. Let $M$ be random matrix whose diagnoal entries are $i.i.d$ samples from $P$ and off-diagonal entries are $i.i.d$ samples from $Q$.
            Choose a permutation $\sigma$ on $[n]$ uniformly at random, permutate columns of $M$ to yield $M_{\sigma}$. Then, we have
            \[
              d_{TV}(M_\sigma,Q^{\otimes n\times n})\le \sqrt{\frac{\chi^2(P,Q)}{2}}.  
            \]
        \end{lem}
        \pfskm{lemma}{\ref{off_diag_permut}}{
            First, note that $d_{\mathrm{TV}}\left(M_\sigma, Q^{\otimes n \times n}\right) \leq \frac{1}{2} \sqrt{\chi^2\left(M_\sigma, Q^{\otimes n \times n}\right)}$. 
            
            Then direct calculation yields
            \begin{align*}
                &\chi^2\left(M_\sigma, Q^{\otimes n \times n}\right)+1
                =\int \frac{\mathbb{E}_\sigma\left[\mathbb{P}_{M_\sigma}(X \mid \sigma)\right]^2}{\mathbb{P}_{Q^{\otimes n \times n}}(X)} d X
                =\mathbb{E}_{\sigma, \sigma^{\prime}} \int \frac{\mathbb{P}_{M_\sigma}(X \mid \sigma) \mathbb{P}_{M_{\sigma'}}\left(X \mid \sigma^{\prime}\right)}{\mathbb{P}_{Q^{\otimes n \times n}}(X)} d X\\
                &= \mathbb{E}_{\sigma, \sigma^{\prime}} \int\left(\prod_{i: \sigma(i)=\sigma^{\prime}(i)} 
                \frac{P\left(X_{i \sigma(i)}\right)^2}{Q\left(X_{i \sigma(i)}\right)}\right)\left(\prod_{i: \sigma(i) \neq \sigma^{\prime}(i)} P\left(X_{i \sigma(i)}\right)\right) 
                 \times\left(\prod_{i: \sigma(i) \neq \sigma^{\prime}(i)} P\left(X_{i \sigma^{\prime}(i)}\right)\right)\left(\prod_{j \neq \sigma(i), j \neq \sigma^{\prime}(i)} Q\left(X_{i j}\right)\right) d X \\
                & =\mathbb{E}_{\sigma, \sigma^{\prime}}\prod_{i: \sigma(i)=\sigma^{\prime}(i)}\left(\int \frac{P\left(X_{i \sigma(i)}\right)^2}{Q\left(X_{i \sigma(i)}\right)} d X_{i \sigma(i)}\right) \\
                & =\mathbb{E}_{\sigma, \sigma^{\prime}}\left(1+\chi^2(P, Q)\right)^{\left|\left\{i: \sigma(i)=\sigma^{\prime}(i)\right\}\right|}
                \le e^{\chi^2(P,Q)} \le 2\cdot\chi^2(P,Q) + 1,
            \end{align*}
            where the last inequality is because $e^x\le1+2x$ for $x\in[0,1]$, and the second to last inequality is because MGF of $Y:=|\{i:\sigma(i)=\sigma'(i)\}|$ is upper bounded by MGF of Poisson distribution with rate $1$,
            therefore $\mb E[e^{tY}]\le e^{e^t-1}$. 
        }
        Now we replace $\mc N(\lambda,1)$ in diagonal entries of $X$ to $\mc N(0,1)$ and permutate the columns with the same permutation as $W$, denote the result matrix by $X_{rep}$. Then,
        use similar proof to lemma \ref{off_diag_permut}, assume $\lambda\le O(1/\sqrt{\log n})$ we have
        \[
            d_{TV}(X_{rep}, X)\le O(\sqrt{\chi^2(N(0,1),N(\lambda,1))})=O(\sqrt{e^{\lambda^2}-1})=O\left(\frac{1}{\sqrt{\log n}}\right).
        \]
        Therefore, $d_{TV}(W, X)\le d_{TV}(W, X_{rep}) + d_{TV}(X_{rep}, X)=d_{TV}(W_{off},X_{off}) + d_{TV}(X_{rep}, X)=O\left(\frac{1}{\sqrt{\log n}}\right)$.
\end{enumerate}
By reduction steps above, for any $k$, as long as $\lambda\le O(1/\sqrt{\log n})$ we obtain a random matrix that almost satisfies distribution of $BC(n,k,\lambda)$ from a random graph of $PC(n,k)$. 
Note that this regime is where $\lambda=n^{-\beta}$ for $\beta\ge 0$. 

Therefore, assume the hardness result of $PC(n,k)$ for $k=n^{\alpha}$ with $\alpha<1/2$, we obtain the hardness of $BC(n,k,\lambda)$ on the line segment $\{(\alpha,0):0<\alpha<1/2\}$.

\vspace{5pt}
Another computational boundary $\{(\alpha,\beta):1/2<\alpha<2/3,\beta=2\alpha-1\}$ can be reduced from $BC(n,n^\alpha,n^{-\beta})$ with $\alpha=1/2,\beta=0$ . 
This is done by the following observation: if we clone the entries of $X$ 4 times to obtain a new random matrix $\tilde{X}\in\rn{2n}$ with $2k\times 2k$ submatrix, then the resulting problem is actually $BC(2n,2k,\lambda/2)$, since the signal shrinks by a factor of $1/\sqrt{2}$ each time it is cloned. 

Surprisingly, note that $\frac{(2k)^2(\lambda/2)}{2 n} = \frac{k^2\lambda}{n}$. Therefore, if $BC(n,k,\lambda)$ is at computational boundary, so does $BC(2n,2k,\lambda/2)$. Then, repeat the procedure for $l$ times, we show $BC(2^ln,2^lk,2^{-l}\lambda):=BC(\tilde{n},\tilde{k},\tilde{\lambda})$ is at computational boundary. In all, let $\tilde{k}=\tilde{n}^{\tilde{\alpha}}$, $\tilde{\lambda}=\tilde{n}^{\tilde{-\beta}}$, starting from the hardness result at $\alpha=1/2,\beta=0$, 
we actually establish the hardness result in the regime
$$\tilde{\alpha} = \log_{\tilde{n}}\tilde{k}=\frac{l\log 2 + \frac{1}{2}\log n}{l\log 2 + \log n},~~~\tilde{\beta}=\log_{\tilde{n}}\tilde{\lambda}=\frac{l\log 2}{l\log 2 + \log n}$$
for any $l\ge 0$, which shows the computational boundary at $\tilde{\beta}=2\tilde{\alpha} -1$ if we take $l=O(\log n)$.
\vspace{10pt}

%---------------------------------------------------
\subsubsection{Further directions}
\textbf{Web of reduction}

We actually have a web of reduction to unified many different problems(see figure \ref{web_of_redu}).
Later in \cite{brennan2020reducibility} the authors construct a novel problem to reduce more problems with different structures(see figure \ref{web_of_redu2}).
\xb{write some intro....}

\begin{figure}
    \centering
    \includegraphics[width=0.55\textwidth]{web_of_redu.jpg}
    \caption{Web of reduction \xb{cite:from brysler's talk}}
    \label{web_of_redu}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{web_of_redu2.jpg}
    \caption{Web of reduction  \cite{brennan2020reducibility}}
    \label{web_of_redu2}
\end{figure}


An open problem is to consider the equivalence of the problem, that is, reduction from both sides.

\textbf{Other related problems and complexities}
\begin{itemize}
    \item Communication complexity
    
    \xb{add some intro...rNLA type of papers usually emphrase model of query, but info-comp gap type of papers ususally don't? check that, e.g. what's query model of hardness conjecture of planted cliques}

    \cite{rashtchian2021averagecase} attemps to make communication problem(alice and bob) as a \textbf{new primitive}, prior to planted clique problem and its variants, under different query models. 
    
    For example, a reduction from planted clique(average-case) $PC(n,k)$ to unique disjointness(worst-case) $UD(l)$(which may be too lose since the primitive is nonrandom):
    \begin{enumerate}
        \item Given inputs $x,y\in\{0,1\}^l$ for Alice and Bob respectively.  Construct a clique partition of $K_n$ with a collection $S$ of $l$ edge-disjoint $k$-cliques, which requires $l=\Theta(n^2/k^2)$.
        \item Introduce randomness. First randomly picked the edges that aren't covered by $S$ with probability $1/2$, denote the resulting graph by $G'$. Then, each of the $k$-cliques $K_k^i$ in $S$  is randomly colored at random with 4 colors, independently of each other.
        \item Construct graph. For each $i\in[l]$, if $x_i=0$, pick all edges in $K_k^i$ with colors $1$ or $3$ as graph $G_1^i$, otherwise pick edges with colors $1$ or $2$.
        If $y_i=0$, pick all edges in $K_k^i$ with colors $1$ or $4$ as graph $G_2^i$, otherwise pick edges with colors $3$ or $4$.

        \item Let $G=G'\cup(\cup_{i\in[l]}G_1^i\oplus G_2^i)$. Then $G\sim\mc H_1$ if there is a unique $i$ such that $x_i=y_i=1$, while $G\sim\mc H_0$ if there is no such $i$. Therefore, if we can solve $PC(n,k)$ under some query model(e.g. $\mb F_2$ sketching), we need at least $\Omega(n^2/k^2)$ queries due to communication complexity.
    \end{enumerate}

    \item Randomized numerical linear algebra
    
    \xb{woodruff's paper usually map problem they care into classical problem(e.g. communication), e.g. Hutch++\cite{mmmw21}. But they don't seem to estabilish system like \cite{brennan2019reducibility}}
    
    \item Graph theory
    
    \xb{search cheng mao, jian ding's works, their problems may be reduce other problem, but first we need to abstract their math structures}

    \item 
\end{itemize}

%---------------------------------------------------
%---------------------------------------------------
\subsection{Failure of general algorithms}

%---------------------------------------------------
\subsubsection{Sum-of-Squares}
This section is mainly based on \cite{sos_1}\cite{sos_2}\cite{sos_schramm}.

When proving the identifiability of a hypothesis testing problem, it is common to use constructive proof by constructing a statistic that demonstrates the identifiability. However, these statistics may not be computable in polynomial time.
For example, the searching statistics $T_{search}=\frac{1}{k}\max_{|S|=|T|=k}\sum_{i\in S,j\in T}X_{ij}$ for biclustering problem \ref{bc_prob} is inefficient\footnote{We usually care about time-efficiency, rather than space-efficiency. See \cite{sos_schramm} lecture 0.}.

The Sum-of-Squares(SOS) method is a novel framework to prove, with construction of poly-time computable instances, identifiability of statistical problems.
By this way, we simultaneously obtain a poly-time algorithm to solve the problem once we prove the identifiability.

More precisely, SOS is a \textbf{system} for proving problems that non-negativity of $p_1,...,p_m$ implies non-negativity of $p$. Denote it by $p_1(x)\ge 0,...,p_m(x)\ge 0\vdash p(x)\ge 0$. We call a SOS proof is \textbf{$d$-degree} if $\max\{\max_{i\in[m]} deg(p_i),deg(p)\}=d$, denoted by $\vdash_d$.


The framework is 
\begin{enumerate}
    \item Translation
    
    Find some (low) degree-$d$ $n$-variable polynomials $p_1(x),p_2(x),...,p_m(x)$ and $p(x)$, $x=(x_1,...,x_n)\in\mb R^n$. Translate the assumptions of the problem into $x\text{ s.t. }p_1(x)\ge 0,...,p_m(x)\ge 0$\footnote{Actually, we may WLOG relax equality constraint to inequality constraint, since $p=q$ is equivalent to $p-q\ge 0$ and $q-p\ge 0$.}, and the objects of the problem into $p(x)\ge 0$.
    Then the statement to prove for the problem is \textbf{translated} to
    \begin{equation}
        \label{sos_fundamental}
        p_1(x)\ge0,...,p_m(x)\ge 0\vdash_d p(x)\ge 0.
    \end{equation}

    \item Verification
    
    Starting from some basic SOS inequalities. Use language of SOS proof to \textbf{verify} \eqref{sos_fundamental}. 
    

    \begin{enumerate}
        \item (Basic inequalities) To prove "$\vdash$", we usually construct a decomposition such that 
        $$p(x)=\sum_{S\subseteq [m]}\prod_{i\in S}p_i(x)\cdot q_S(x)+g(x),$$ 
        where $q_S,g$ are sum-of-squares decompositions $q_S(x)=\sum_{j}q_{S,j}^2(x)$ and $g(x)=\sum_j g_j^2(x)$.
        This is to say, we use sum-of-squares polynomials to prove non-negativity arguments. 
        By this way, if $p-q$ is sum-of-squares, we also call the inequalities \textbf{``SOS less than"}, and denote them by "$q\preccurlyeq p$" instead.
        
        
        Note that, inequality of polynomial $\mb R[x]\ni p\le q\in\mb R[x]$ may not always have SOS of polynomial, $i.e.$ $q-p=SOS$, but can be written as SOS of rational function(see \cite{sos_schramm} lecture 0).

        \item (Steps of induction) Then, we operate the inequalities in this proof system. 
        
        For example,  $a(x)\ge 0\vdash b(x)\ge 0$, then $a(x)p^2(x)\ge 0\vdash b(x)p(x) + q^2(x)\ge 0$.
        
        

    \end{enumerate}

    \item Solution
    
    Once \eqref{sos_fundamental} is verified, the statement of the SOS system yields a poly-time algorithm by the main theorem \ref{sos_main}.
    
    \begin{thm}
    \label{sos_main}
        There is a $(n\cdot m)^{O(d)}$ time algorithm to solve SOS system \eqref{sos_fundamental}.
    \end{thm}
    
    The proof is left to further sections.

   

\end{enumerate}

\bigskip



\textbf{Example: robust mean estimation}

\begin{prob}
\label{robust_mean_prob}
Given distribution $\mc D$ on $\mb R^d$ and vector $\mu\in\mb R^d$, such that
$\mb E_{X\sim\mc D}[X]=\mu$ and $\mb E_{X\sim\mc D}(X-\mu)(X-\mu)^T=\Sigma\preccurlyeq C\cdot I$ for some constant $C>0$. 
An adversary modify $\epsilon$-proportion of samples $X_1',...,X_n'\stackrel{\mathrm{i.i.d.}}{\sim}\mc D$, yielding corrupted samples $X_1,...,X_n$ as observation.
Use corrupted samples to estimate $\mu$ with small error. 
\end{prob}

Here we want to know the \textbf{identifiability} of this problem. 
That is, whether it is possible to estimate $\mu$ from the $\epsilon$-corruption observation or not. 
This is in essence a signal-noise tradeoff problem, and
we first formulate a sufficient condition of identifiability by classical way, that is:

\begin{thm}
    \label{robust_mean_classic_main}
    If \begin{enumerate}
        \item (Strong signal) $n\gg poly(d/\epsilon)$,
        \item (Weak noise) there exists an $(1-\epsilon)n$-subset $S\subseteq[n]$ such that, the empirical covariance matrix over $\{X_i\}_{i\in S}$ is bounded,
    \end{enumerate}
    then the problem is identifiable. Moreover, there exists estimator $\hat{\mu}~s.t.~\|\mu-\hat{\mu}\|\le O(\sqrt{\epsilon})$.
\end{thm}

We will see, the classical way of proof can be constructive and yield an algorithm to calculate $\hat{\mu}$ if the problem is identifiable. However, \textbf{the running time for outputting $\hat{\mu}$ may be exponentially long}. On the other hand, SOS proof yields poly-time algorithm.
Assume $n\gg poly(d/\epsilon)$, we have
\begin{itemize}
    \item Classical proof: $~~~~O(\binom{n}{n\epsilon})~~~~~~~~$ time.
    \item SOS proof: $~~~~~~~~~[d(d+n)]^{O(1)}~$ time.
\end{itemize}

\pf{\ref{robust_mean_classic_main}}{
\begin{enumerate}
    \item First, we show \textbf{how strong signal} will the \textbf{true} samples give us. 
    
    Let $\bar{X'} = \frac{1}{n}\sum_{i=1}^n X_i'$ and $\Sigma_X'=\frac{1}{n}\sum_{i=1}^n(X_i'-\bar{X'})(X_i'-\bar{X'})^T$, the concentration results yields
    \begin{lem} For any $\epsilon>0$, if $n=\Omega(poly(d/\epsilon))$, we have
        \label{empi_concen}
        \begin{enumerate}
            \item $\|\mu - \bar{X'}\|\le \sqrt{\epsilon}~~w.h.p$,
            \item $\|\Sigma_X'-\Sigma\|\le  C~~w.h.p$ for some constant $C>0$, and thus $\Sigma_X'\preccurlyeq C\cdot I~~w.h.p.$
        \end{enumerate}
    \end{lem}
    \pfskm{lemma}{\ref{empi_concen}}{
        \begin{enumerate}
            \item For any constant $C_1>0$, 
                \begin{align*}
                    \mb P(\|\mu-\bar{X'}\|\ge C_1\sqrt{\epsilon}) 
                    \le \mb P\left(\exists j\in[d] \text{ s.t. } \left|\mu_j-\frac{1}{n}\sum_{i=1}^n X_{i,j}' \right|\ge C_1\sqrt{\frac{\epsilon}{d}}\right)
                    \le \sum_{j=1}^d \frac{O_n(C/n)}{C_1^2 \epsilon/d}
                    = O_n\left(\frac{d^2}{\epsilon}\cdot \frac{1}{n}\right).
                \end{align*}
            \item Since $\Sigma_X' = \frac{1}{n}\sum_{i=1}^n(X_i'-\mu)(X_i'-\mu)^T - (\mu - \bar{X'})(\mu - \bar{X'})^T.$
                By proof above we know $\|(\mu - \bar{X'})(\mu - \bar{X'})^T\|=\|\mu - \bar{X'}\|^2\le \epsilon~~w.h.p.$ Then, under some regularity conditions,
                we have $\left\|\frac{1}{n}\sum_{i=1}^n(X_i'-\mu)(X_i'-\mu)^T-\Sigma\right\|\le \epsilon~~w.h.p$, and thus $\|\Sigma_X'\|\le \|\Sigma\| + 2\epsilon\le C~~w.h.p$ for some constant $C>0$.
                
                For example, if we assume boundedness of $\|X_1'-\mu\|$, by matrix bernstein inequalities we have
                \begin{align*}
                    \mb P\left(\left\|\frac{1}{n}\sum_{i=1}^n(X_i'-\mu)(X_i'-\mu)^T-\Sigma\right\|\ge \epsilon\right) \le 2d \exp\left\{-O\left(\frac{n\epsilon^2}{\|\Sigma\|+\epsilon}\right)\right\}.
                \end{align*}
                Note that there exists other condition such as boundedness of the fourth moment.
        \end{enumerate}
    }
    This means, when $n$ is sufficiently large, \textbf{signal} from empirical mean of true samples is \textbf{strong} enough for estimation. However, the true samples are contaminated and some information is lost.

    \item Then, we control the \textbf{noise} due to $\epsilon$-corruption. 
    
    Note that, $\epsilon$-corruption means $\epsilon$-total variation distance of empirical distributions of true and corrupted samples.
    That is, $d_{TV}(\mc D_X', \mc D_X)\le \epsilon$.
    Intuitively, if the variance are bounded, then small total variation distance implies closeness of the means, 
    since \textbf{boundedness of variance implies the differences between two distribution are not so wild} and thus \textbf{the noise is weak}.
    Therefore, empirical mean $\bar{X}$ of corrupted samples $X$ may be close to empirical mean $\bar{X'}$ of true samples $X'$.

    Formally, 
    \begin{lem}
    \label{robust_mean_clossic_sol}
    Let $X,Y$ be random vectors on $\mb R^d$, such that $\mb E(X-\mb E X)(X-\mb E X)^T\preccurlyeq C\cdot I$ and $\mb E(Y-\mb E Y)(Y-\mb E Y)^T\preccurlyeq C\cdot I$ for some constant $C>0$.
    If $d_{TV}(X,Y)\le \epsilon$, then $\|\mb E X-\mb E Y\|\le O(\sqrt{\epsilon})$.
    \end{lem}
    \pfskm{lemma}{\ref{robust_mean_clossic_sol}}{
    By lemma \ref{tv_coup}, take coupling over $(X,Y)$ such that $\mb P(X\neq Y)\le\epsilon$. Then, 
    \begin{equation}
    \label{cauchy_schwarz_les}
    \begin{split}
        &\|\mb EX-\mb E Y\|^2 = \langle \mb E(X-Y)\mf{1}_{X\neq Y}, \mb E X - \mb E Y \rangle
        = \mb E \mf{1}_{X\neq Y} \langle X - Y, \mb E X - \mb E Y \rangle\\
        &\stackrel{(a)}{\le} \sqrt{\mb E \mf{1}_{X\neq Y}^2} \sqrt{\mb E \langle X - Y, \mb E X - \mb E Y \rangle^2}
        \le \sqrt{\epsilon} \cdot \sqrt{\mb E \langle X - Y, \mb E X - \mb E Y \rangle^2},
    \end{split}
    \end{equation}
    where $(a)$ uses Cauchy-Schwarz inequality.
    Then, 
    \begin{equation}
    \label{sos_les}
    \begin{split}
        &\mb E \langle X - Y, \mb E X - \mb E Y \rangle^2 
        = \mb E\left[  
            \langle X - \mb E X, \mb E X - \mb E Y \rangle 
            -  \langle Y - \mb E Y, \mb E X - \mb E Y \rangle
            + \langle \mb E X - \mb E Y, \mb E X - \mb E Y \rangle
        \right]^2\\
        &\stackrel{(b)}{\le} O(1)\cdot \left[
            \mb E \langle X - \mb E X, \mb E X - \mb E Y \rangle^2
            + \langle Y - \mb E Y, \mb E X - \mb E Y \rangle^2
            + \langle \mb E X - \mb E Y, \mb E X - \mb E Y \rangle^2
        \right]\\
        &\stackrel{(a)} O(1)\cdot\left[
            \mb E\| X - \mb E X \|^2 \cdot \|\mb E X - \mb E Y\|^2
            + \mb E\| Y - \mb E Y \|^2 \cdot \|\mb E X - \mb E Y\|^2
            + \|\mb E X - \mb E Y\|^4
        \right]\\
        &\stackrel{(c)}{\le} O(1)\cdot [ \|\mb E X - \mb E Y\|^2 +  \|\mb E X - \mb E Y\|^4],
    \end{split}
    \end{equation}
    where $(b)$ uses $2ab\le a^2 + b^2$ and $(c)$ uses boundedness of variance.
    Therefore, we have
    \begin{equation}
    \label{comb_les}
    \begin{split}
    & \|\mb EX-\mb E Y\| \le O(\sqrt{\epsilon})\sqrt{1 + \|\mb EX-\mb E Y\|^2}
    \le O(\sqrt{\epsilon})(1 + \|\mb EX-\mb E Y\|)\\
    \Longrightarrow & \|\mb EX-\mb E Y\| \le O(\sqrt{\epsilon}).
    \end{split}
    \end{equation}
    }
    
    \item Finally, it suffices to show empirical covariance $\Sigma_X$ of corrupted samples is bounded, since we already show empirical covariance $\Sigma_X'$ of true samples is bounded in lemma \ref{empi_concen}, and $d_{TV}(\mc D_X', \mc D_X)$ is small. 
    However, since the modification can be very wild on the corrupted sample, we may not have $\Sigma_X\preccurlyeq O(1)\cdot I$. 
    
    Fortunately, the \textbf{assumption} that $\Sigma_S=\frac{1}{|S|}\sum_{i\in S}\left(X_i-X_S\right)\left(X_i-X_S\right)^T\preccurlyeq C\cdot I$ for some constant $C>0$, 
    where $X_S=\frac{1}{|S|}\sum_{1\in S}X_i$, reduces the wildness of the corruption.  Then we can use $X_S$ as an estimator instead of $\bar{X'}$. It suffices to show empirical distirbution $\mc D_S$ over $\{X_i\}_{i\in S}$ are not so far from $\mc D_X'$ $w.r.t.$ total variation distance.

    Indeed, we have $d_{TV}(\mc D_X',\mc D_S)\le d_{TV}(\mc D_X',\mc D_X) + d_{TV}(\mc D_X,\mc D_S)\le 2\epsilon$. Then we apply lemma \ref{robust_mean_clossic_sol} to distribution $\mc D_X'$ and $\mc D_S$ to yield
    $\|\bar{X'}-X_S\|\le O(\sqrt{\epsilon})~~w.h.p.$, and thus $\|X_S-\mu\|\le O(\sqrt{\epsilon})~~w.h.p.$.


\end{enumerate}
}

This constructive proof yields a naive algorithm to calculate $\hat{\mu}$.
\bigskip
\begin{breakablealgorithm}
    \caption{Naive algorithm for robust mean estimation}
    \label{ineff_robu_mean_est}
    \begin{algorithmic}[1]
    \Require{$\epsilon$-corruption Samples $X_1,...,X_n$.}
    \Ensure{Mean estimation $\hat{\mu}$ such that $\|\hat{\mu}-\mu\|\le O(\sqrt{\epsilon})$.}
    \For{$S\subseteq[n]$ with $|S|=(1-\epsilon)n$}
        \State Calculate $X_S=\frac{1}{|S|}\sum_{1\in S}X_i$ 
            and $\Sigma_S=\frac{1}{|S|}\sum_{i\in S}(X_i-X_S)(X_i-X_S)^T$.
        \If{$\|\Sigma_S\|\le 2$}
            \State{Return $\hat{\mu}=X_S$}
        \EndIf
    \EndFor
    \end{algorithmic}
\end{breakablealgorithm}

\bigskip
However, the algorithm is inefficient, since the \textbf{running time of brutal search is $O(\binom{n}{n\epsilon})$}.
Surprisingly, with some translation from this direct proof, we can reformulate the problem into SOS framework. The constructive proof of SOS yields a \textbf{poly-time} computable algorithm to calculate $\hat{\mu}$.

\pfm{SOS version of Theorem}{\ref{robust_mean_classic_main}}{
\hfill
\begin{enumerate}
    \item Translation
    
    First we translate the assumptions into non-negativity of multivariate polynomials.
    
    Introduce variables $y\in\mb R$, $y_1,...,y_n\in\mb R^d$, indicator variables $w_1,...,w_n$ and auxiliary variables $B\in\rn d$.
    From the previous analysis, we need these variables satisfying constraint below, denoted by $\mc A$:
    
    \begin{enumerate}
        \item ($\epsilon$-corruption) Find an $(1-\epsilon)n$-subset of corrupted samples.
        \begin{itemize}
            \item $w_i^2=w_i$ for $i\in[n]$,
            \item $w_i X_i= w_i y_i$ for $i\in[n]$,
            \item $\sum_{i=1}^n w_i=(1-\epsilon)n$.
        \end{itemize}
        
        \item (Bounded variance) Corruption on that subset is not so wild.
        \begin{itemize}
            \item $C\cdot I-Cov(y_1,...,y_n)=BB^T$ for some constant $C>0$.
        \end{itemize}
        \item (Estimator) Another variable for estimation
        \begin{itemize}
            \item $y=\frac{1}{n}\sum_{i=1}^n y_i$.\footnote{Though it's slightly different from original condition(only consider samples over subset $S$), letting $y_i=0$ for $i$ s.t. $w_i=0$ we obtain the same modeling as classicl proof there. Actually, both settings are equivalent.
            }
        \end{itemize}
    \end{enumerate}

    There are $n+d\cdot n + 1 + d\cdot d + 1$ equations of polynomials in total. For each equation $p=q$, we actually obtain two inequalities $p-q\ge 0$ and $q-p\ge 0$. Therefore the original assumptions now become $p_1(x)\ge 0,...,p_m(x)\ge 0$ with $x=(y,y_1,...,y_n,w_1,...,w_n,B)\in\mb R^{1+(d+1)n+d^2}$ and $m=2[(d+1)n+d^2+2]$.
    
    Similarly, the objects of the problem become $p(x)=O(\epsilon)-\|y-\mu\|_2^2\ge 0$. Therefore the statement is
    \begin{equation}
        \label{robust_mean_est_sos}
        p_1(x)\ge 0,...,p_m(x)\ge 0 \vdash p(x)\ge 0~~~~~\text{or}~~~~~~\mc A\vdash p(x)\ge 0.
    \end{equation}

    \item Verification
    
    Then, prove \eqref{robust_mean_est_sos}.
    \begin{enumerate}
        \item (Basic inequalities) From the proof of lemma \ref{robust_mean_clossic_sol}, we only need the following two inequalities
        \begin{itemize}
            \item $(a+b+c)^2\preccurlyeq O(1)\cdot (a^2+b^2+c^2)$. 
            
            This is because $a^2+b^2-2ab=(a-b)^2$, then we have $2ab\preccurlyeq a^2+b^2$(or $2ab\preccurlyeq a^2+b^2$), and then $(a+b)^2\preccurlyeq 2(a^2+b^2)$(or $(a+b)^2\preccurlyeq 2(a^2+b^2)$).
            
            \item (Cauchy-Schwarz) $\left(\sum_{i=1}^n a_i b_i \right)^2\preccurlyeq \left(\sum_{i=1}^n a_i^2\right) \left(\sum_{i=1}^n b_i^2\right)$.
            
            This is because $\left(\sum_{i=1}^n a_i^2\right) \left(\sum_{i=1}^n b_i^2\right) - \left(\sum_{i=1}^n a_i b_i \right)^2 = \sum_{i<j}^n(a_ib_j - a_jb_i)^2$.

        \end{itemize}
        
        \item (Steps of induction) Let $X$ satisfies empirical distribution of $X_i$, denoted by $\mc D_X$, $X'$ satisfies empirical distribution of $X_i'$, denoted by $\mc D_X'$, and $Y$ satisfies empirical distribution of of $y_i$, denoted by $\mc D_Y$. Also denote empirical distribution of true smaples $X_i'$ by $\mc D_X'$. Then all the expectations in the proof of lemma \ref{robust_mean_clossic_sol} are polynomials.
        For example, $\mb EY= \frac{1}{n}\sum_{i=1}^n y_i=y$ is degree-$1$ polynomial.

        If we obtain a feasible solution $x=(y,y_1,...,y_n,w_1,...,w_n,B)$ satisfying all the constraint in the step 1, then we already have $Cov(y_1,...,y_2)\preccurlyeq 2I$  and  $d_{TV}(\mc D_X,\mc D_Y)\le \epsilon$. As long as $n\gg poly(d/\epsilon)$ for concentration of $X_i$, all the conditions of lemma \ref{robust_mean_clossic_sol} are satisfied,
        and we further have $d_{TV}(\mc D_X',\mc D_Y)\le 2\epsilon$ and $Cov(X_1',...,X_n')\preccurlyeq C\cdot I$. Now it suffices to modify its proof into SOS framework. 
        
        By \eqref{cauchy_schwarz_les} we have
        \begin{equation}
        \begin{split}
            &\|\mb EX'-\mb E Y\|^4 \preccurlyeq \epsilon \cdot \mb E \langle X' - Y, \mb E X' - \mb E Y \rangle^2~~~~w.h.p.\text{( of $X_i'$)}.
        \end{split}
        \end{equation}
        By \eqref{sos_les} we have
        \begin{equation}
        \begin{split}
            \mb E \langle X' - Y, \mb E X' - \mb E Y \rangle^2 \preccurlyeq O(1)\cdot [ \|\mb E X' - \mb E Y\|^2 +  \|\mb E X' - \mb E Y\|^4]~~~~w.h.p.\text{( of $X_i'$)}.
        \end{split}
        \end{equation}
        Therefore, $\|\mb EX'-\mb E Y\|^4 \preccurlyeq O(\epsilon)\cdot \|\mb EX'-\mb E Y\|^2~~w.h.p.$, and thus we have $\|\bar{X'}-y\|^4\preccurlyeq O(\epsilon) \|\bar{X'}-y\|^2~~w.h.p.$, where $\bar{X'}=\frac{1}{n}\sum_{i=1}^n X_i'$.
        
        Note that, max degree of all the polynomials appeared in the proof above is $6$, thus it is a degree-$6$ SOS proof. Therefore we have
        \begin{equation}
        \label{robust_mean_est_sos_final}
            \mc A \vdash_6 \|\bar{X'}-y\|^4\preccurlyeq O(\epsilon) \|\bar{X'}-y\|^2~~~~w.h.p.\text{( of $X_i'$)}.
        \end{equation}
        Though the $RHS$ is slightly different from $p(x)$ defined in \eqref{robust_mean_est_sos}, the two polynomial are equivalent, since $\|\mu-\bar{X'}\|\le \sqrt{\epsilon}~~w.h.p.$ by concentration.

    \end{enumerate}

    \item Solution
    
    Finally, note that $m=2[(d+1)n+d^2+2]=O(d(d+n))$ and dimension of $x$ is $1+(d+1)n+d^2=O(d(d+n))$.
    By theorem \ref{sos_main}, as long as $n=poly(d/\epsilon)$, we only need \textbf{at most $[d(d+n)]^{O(1)}$ running time} to solve \eqref{robust_mean_est_sos_final}.
    Later we will see the detail implementation by SDP.
\end{enumerate}
}

\bigskip

\textbf{Rigorous proof of main theorem \ref{sos_main}}

Detailed implementation of SOS algorithm is based on a definition called pseudoexpectation
\begin{defn}[pseudoexpectation\cite{sos_schramm}]
    \label{pseudo_expe}
    For a set of polynomial axioms $\mathcal{A}=\{p_1\ge 0,...,p_m\ge 0\}$, we say that $\widetilde{\mathbb{E}}: \mathbb{R}[x] \rightarrow \mathbb{R}$ is a degree-$d$ pseudoexpectation satisfying $\mc A$ if it is a linear operator with the following properties:
    \begin{enumerate}
        \item Scaling: $\widetilde{\mathrm{\mb E}}[1]=1$
        \item Respecting axioms: $\widetilde{\mathbb{E}}\left[a^2 p_i\right] \geqslant 0$ for all $i \in[m]$ and $b \in \mathbb{R}[x]$ satisfying $\operatorname{deg}\left(a^2 p_i\right) \leqslant d$.
        \item Non-negativity of squares: $\widetilde{\mathrm{\mb E}}\left[h^2\right] \geqslant 0$ for any polynomial $h \in \mathbb{R}[x]$ with $\operatorname{deg}(h) \leqslant d / 2$
    \end{enumerate}

\end{defn}
Given a constraint set $\mc A$, we need to find a pseudoexpectation $\tilde{\mb E}$ $w.r.t.$ $\mc A$, since it transform the SOS language of inequalities into actual quantities.
For example, for any $s\le d$, if we have $\mc A\vdash_s p\preccurlyeq q$, then $\tilde{\mb E}[p]\le \tilde{\mb E}[q]$.

In robust mean estimation, by the non-negativity of $\widetilde{\mathbb{E}}$ applied to squares, we have
$$
0 \le \widetilde{\mathbb{E}}\left[\left(\|\bar{X'}-y\|^2-\widetilde{\mathbb{E}}\left[\|\bar{X'}-y\|^2\right]\right)^2\right]=\widetilde{\mathbb{E}}\left[\|\bar{X'}-y\|^4\right]-\widetilde{\mathbb{E}}\left[\|\bar{X'}-y\|^2\right]^2 \leqslant \widetilde{\mathbb{E}}\left[\|\bar{X'}-y\|^2\right]\left(O(\varepsilon)-\widetilde{\mathbb{E}}\left[\|\bar{X'}-y\|^2\right]\right),
$$
which implies that $\widetilde{\mathbb{E}}\left[\|\bar{X'}-y\|^2\right] \leqslant O(\varepsilon)$. Similarly, $\|\bar{X'}-\widetilde{\mathbb{E}}[y]\|^2 \leqslant O(\varepsilon)$. So the quantity $\widetilde{\mathbb{E}}[y]$ computed by our algorithm is a good estimate for $\bar{X'}$ and thus for $\mu$.

To find a pseudoexpectatio $w.r.t.$ $\mc A$, by linearity it suffices to prescribed pseudoexpectation $\wtd{\mb E}[x_S]$ for any degree-$d$ monomials $x_S=\prod_{i\in S\subseteq[n],|S|\le d}x_i$. Therefore, consider a large matrix $Z\in\rn{n^{O(d)}}$, whose rows and columns are indexed by $d$-subsets of $[n]$. Then, we may take $\wtd{\mb E}[x_{S}]=Z_{A,B}$, where $A\cup B=S$ is an arbitrary partition of $S$. To make it well-defined, assume $Z_{\emptyset,\emptyset}=1$ and $Z_{A,B}=Z_{S,T}$ if $A\cup B=S\cup T$. 

Now, we need to make sure non-negativity and axiom respecting, The way to find such matrix $Z$ is SDP, problem \ref{sdp}.
\begin{prob}[\cite{sos_schramm}]
    \label{sdp}
    Given matrix $C, P_i\in\rn r$, $i\in[m]$, solve the following optimization problem
    \begin{equation}
    \begin{split}
        \max_Z~~~~&\langle Z,C\rangle \\
        s.t.~~~~ &Z \succeq 0\\
        &\left\langle Z, P_i\right\rangle\ge 0~~\forall i\in[m]
    \end{split}
    \end{equation}
    Set $C=0$, the problem become finding a feasible solution of the constraints.
\end{prob}
The state-of-art SDP algorithm, such as interior point method and epllisoid method, will solve this system in time $poly(r,m)$.

The steps to find a suitable $Z$ as basis of pseudoexpectation by SDP are as follows
\begin{enumerate}
    \item First, transform the scalar constraint in $\mc A$ into matrix inner-produt.
    
    Indeed, for any $i\in[m]$, $\wtd{\mb E}[p_i]\ge 0$ is actually $\langle Z, P_i \rangle \ge 0$, where $(P_i)_{S,T} = \hat{p_i}(S\cup T)$, which is the coefficient of monomials $x_{S\cup T}$ in $p_i$. To make $\wtd{\mb E}[a^2 p_i]\ge0$ for all $a(x)\in\mb R_{\le d/2}[x]$, it suffices to consider $\wtd{\mb E}[x_S^2 p_i]\ge 0$ for all $S\subset[n],|S|=d/2$. Therefore, we transform $m$ equality constraint in $\mc A$ into $m\cdot n^{O(d)}$ equality constraint in SDP.

    \item Then, the non-negativity is trivial since $0\preccurlyeq Z$ and $\wtd{\mb E}[h^2]= \hat{h}^T Z \hat{h}\ge 0$, where $\hat{h}$ is vector of coefficient of all monomials in $h$. 
    
    \item Finally, use state-of-art SDP algorithm, we will solve the SOS algrotihm in time $poly(n^{O(d)}, m\cdot n^{O(d)}) = (n\cdot m)^{O(d)}$.
\end{enumerate}


\bigskip

\textbf{Limitation}

\begin{enumerate}
    \item Are polynomials enough?

    Not all the problems can be translated into non-negativity of polynomials. (LDP has similar issues.) For example, calculate $\frac{Av}{\|Av\|}$ in power method.
    
    \item Inequalities need SOS version
    
    Not all polynomial inequalities $p\ge q$ has SOS formulation $p-q=\sum_i g_i^2$. But this is significant for efficiency of SOS, since we need to construct pseudoexpectation inequality $\td{\mb E}[p]\ge \td{\mb E}[q]$ by SDP. 
    Time complexity of the SDP exponentially depends on dimension of function space of $g_i$, since we constrain $\td{\mb E}[h^2]\ge 0~\forall h\in\mb R[x]$. Space of low degree polynomials is low-dimensional.

\end{enumerate}

\bigskip

\textbf{Hardness and lower bound}

\xb{see \cite{barak2016nearly} and more}

\bigskip

\textbf{More problems}

\xb{see xiechuhan's notes and more sos papers and \cite{barak2016nearly}}

\bigskip




\textbf{Relatioship with low degree polynomial method}

\xb{see [Hop18],[KWB19] and more}






%---------------------------------------------------
%---------------------------------------------------
\subsection{Phenomena in hard regime}