\chapter{Theory for areas}
General complexity theory of specific areas of problems. 

%---------------------------------------------------
%---------------------------------------------------
\section{Matrix-vector multiplication queries}
We interact with the algorithm only through inputting query vectors and outputting matrix-vector product oracles. 

%---------------------------------------------------
\subsection{Trace estimation}
\begin{prob}[trace estimation\cite{mmmw21}]
\label{tr_est}
Given a matrix $A$, we input query vectors $r_1,...,r_m$ and output product $Ar_1,...Ar_m$ to estimate $tr(A)$. 
\end{prob}


There are a two types of query vectors.
\begin{enumerate}
    \item (Adaptive) Vectors $r_1,...,r_m$ are chosen adaptively. For example, $r_2=Ar_1$. In this case, $A$ may be used for several times.
    \item (Non-adaptive) Vectors $r_1,...,r_m$ are chosen independently. In this case, $A$ can be used for only once, for example, streaming data.
\end{enumerate}

\begin{thm}[Lower bound of adaptive queries\cite{mmmw21}]
\label{tr_est_ada}
    Let $A$ be a PSD matrix. If $r_1,...,r_m$ are adaptive and with integer entries in $\{-2^b,...,2^b\}$, then we need at least $m=\Omega\bbr{\frac{1}{\epsilon(b+\log(1/\epsilon))}}$ queries to output an estimate $t$ so that, with probability at least $2/3$, $(1-\epsilon)tr(A)\le t\le (1+\epsilon)tr(A)$.
\end{thm}
\pfsk{\ref{tr_est_ada}}{
First, reduce the problem to Gap-hamming distance problem(GHD) \ref{ghd}. For any vector $s,t\in\mb R^n$, reshape $s,t$ to matrix $S,T\in\R^{n\times n}$. Let $Z=S+T$, $A=Z^TZ$. 

Then, if we estimate $tr(A)$ with error $\epsilon=1/\sqrt{n}$, we have
\begin{align*}
    &\widehat{tr(A)}\ge2(n+\sqrt{n})(1-1/\sqrt n)=2n-2 ~~~if~~\langle s,t\rangle\ge\sqrt n,\\
    &\widehat{tr(A)}\le2(n-\sqrt{n})(1+1/\sqrt n)=2n-2 ~~~if~~\langle s,t\rangle\le-\sqrt n,
\end{align*}
which means we can solve GHD problem by simply compare $\widehat{tr(A)}$ with $2n-2$.

Finally, use lower bound of GHD. The reduction above cost $O(m\sqrt{n}(\log n + b))=\Omega(n)$ bits by Theorem \ref{ghd_lower_bound}, thus we have $m=\Omega\bbr{\frac{1}{\epsilon(b+\log(1/\epsilon))}}$.
}

\begin{thm}[Lower bound of non-adaptive queries\cite{mmmw21}]
\label{tr_est_nada}
    Let $A$ be a PSD matrix. If $r_1,...,r_m$ are non-adaptive, then we need at least $m=\Omega\bbr{\frac{1}{\epsilon}}$ queries to output an estimate $t$ so that, with probability at least $3/4$, $(1-\epsilon)tr(A)\le t\le (1+\epsilon)tr(A)$.
\end{thm}
\pfsk{\ref{tr_est_nada}}{
First, solving the problem implicitly solves a testing problem with $m$ matrix-vector product oracles. 
\begin{prob}[\cite{mmmw21}]
\label{tr_est_nada_med}
    Fix $d,n\in\mb N$ such that $d\ge n$ and $n=1/\epsilon$ for $\epsilon\in(0,1]$. Let $A=G^TDG\in\R^{d\times d}$, where $G\in\R^{n\times d}$ generated by $i.i.d$ $\mc N(0,1)$ and $D=I_n:=D_1$ or $\begin{bmatrix} I_{n-1} & 0 \\ 0 & 0 \\ \end{bmatrix}:=D_2$. Consider any algorithm taking a query matrix $U\in\R^{d\times m}$ as input and outputting product oracle $AU\in\R^{d\times m}$ to identify $D$.
\end{prob}
By concentration inequality \ref{hutchinson_bound} of Hutchinson's estimator, we have that with probability at least $11/12$,
\begin{align*}
    \frac{1}{d}tr(G^TD_1 G)\ge (1-\epsilon/4)tr(D_1) ~~~and ~~~ \frac{1}{d}tr(G^TD_2 G)\le (1+\epsilon/4)tr(D_2).
\end{align*}
Then, if we estimate $t\approx tr(A)$ with probability at least $3/4$ and error $\epsilon/4$, we have
\begin{align*}
    & t\ge (1-\epsilon/4)tr(A)\ge (1-\epsilon/4)^2 d/\epsilon\ge(1-\epsilon/2)d/\epsilon ~~if~D=D_1,\\
    & t\le (1+\epsilon/4)tr(A)\le (1+\epsilon/4)^2 (1-\epsilon) d/\epsilon\le(1-\epsilon/2)d/\epsilon ~~if~D=D_2,
\end{align*}
which means we can identify $D$ by simply compare $t$ with $(1-\epsilon/2)d/\epsilon$. By union bound, the probability is at least $2/3$.


Then, this problem can be further reduced to problem \ref{spiked_wishart}. Let $M=\begin{bmatrix}N^TN \\ L^T N\end{bmatrix}$, where $N\indep L$ and $L\in\R^{n\times(d-m)}$ with $i.i.d$ $N(0,1)$. Then we have $M\stackrel{\mathrm{dist}}{=} G^TDGE_m$, where $E_m=[e_1,...,e_m]$ is the first $m$ standard basis vectors. Therefore, it suffices to run algorithm above and if the output $D=D_1$ we claim $C=I$ otherwise $C=I-zz^T$.

% By rotational invaricance of standard Gaussian vectors, WLOG $U=E_m=[e_1,...,e_m]$, the first $m$ standard basis vectors.

% For any uniformly random unit vector $z\in\R^n$, extend it to a uniformly random orthogonal matrix $Z\in\R^{n\times n}$. Let $T=ZDS$ with $S\in\R^{n\times m}$ be $i.i.d$ $\mc N(0,1)$. Then $N \stackrel{\mathrm{dist}}{=} T$ with $D=D_1$ or $D_2$. 
% Thus $N^TN\stackrel{\mathrm{dist}}{=} S^TDZ^TZDS= G_m^T D G_m$. Let $L\in\R^{n\times(d-m}$



Finally, use lower bound theorem \ref{spiked_whishart_lower_bound}, we have $m=\Omega\bbr{\frac{1}{\epsilon}}$.
}

\begin{rmk}
\begin{enumerate}
    \item Lower bound of adaptive case is smaller than non-adaptive case. It make senses since adaptive queries contain more information on $A$.
    \item The difference of two bounds is mathematically because intermediate problem \ref{tr_est_nada_med} requires oracles(columns of $U$) are non-adaptive. But GHD problem doesn't have any requirement on relationships between oracle $r$ s.
    \item This inspires us, to reduce a problem to get a lower bound, we must keep key structures of problems as many as possible. 
    
    For instance, although we can reduce the non-adaptive scenario to GHD, the resulting bound is significantly weaker. This is because we abandon the critical constraint of exclusively using independent oracles $r$ s to tackle the problem, which renders it more challenging.
\end{enumerate}
\end{rmk}

\xb{check why are lower bounds only suitable for PSD matrix.}

Later, \cite{jpwz21} improve the lower bound with error probability $\delta$.
\begin{thm}[Lower bound of non-adaptive queries with $\delta$-error\cite{jpwz21}]
\label{tr_est_nada_improve}
Let $A$ be a PSD matrix. If $r_1,...,r_m$ are non-adaptive, then we need at least $m=\Omega\bbr{\frac{\sqrt{\log(1/\delta)}}{\epsilon}+\frac{\log(1/\delta)}{\log\log(1/\delta)}}$ queries to output an estimate $t$ so that, with probability at least $1-\delta$, $(1-\epsilon)tr(A)\le t\le (1+\epsilon)tr(A)$.
\end{thm}
\pfsk{\ref{tr_est_nada_improve}}{
The proof is divided into two parts according to whether $\epsilon$ is small or large.

\noindent First, for small $\epsilon=O(1/\sqrt{\log(1/\delta)})$ we have $m=\Omega\bbr{\frac{\sqrt{\log(1/\delta)}}{\epsilon}}$:
\begin{enumerate}
    \item Use Yao's principle(theorem \ref{yaoprin}) we only need to find a hard distribution. Since the bound is required to satisfied by any size $n$ of matrix $A\in\R^{n\times n}$, we consider large $n$. Use theorem \ref{block_conditional} to quantify the remaining randomness after we take several matrix-vector product oracles, we can observe if the number of queries is not so large, then the remaining randomness will yield large variance and thus large error probability. 
    \item Specifically, let $W=I_n+\frac{1}{2C\sqrt{n}}(G+G^T)$ where entries of $G$ are $i.i.d.$ $\mc N(0,1)$. Then, $W$ is PSD by theorem \ref{rmt_smax_concen} and $tr(W)\le 2n$ $w.h.p.$ for large $n$, since $tr(G)\sim\mc N(0,n)$. Quantitatively, for any $\delta\in(0,1)$, take $n=\Omega(\log(1/\delta))$ then $tr(W)\le 2n$ with probability at least $1-\delta/10$. Then, for any $\epsilon=O(\frac{\sqrt{\log(1/\delta)}}{n})=O(1/\sqrt{\log(1/\delta)})$, if we just take $m=n/2=O\bbr{\frac{\sqrt{\log(1/\delta)}}{\epsilon}}$, use theorem \ref{block_conditional} we prove the best estimation precision any algorithm can achieve $w.h.p.$ is at most $c\sqrt{\log(1/\delta)}$ for $c=\Omega(1)$, which is large than required precision $\epsilon$ and thus yield lower bound $m=\Omega\bbr{\frac{\sqrt{\log(1/\delta)}}{\epsilon}}$.
\end{enumerate}

\noindent Then, for any $\epsilon\in(0,1)$, we have uniformly lower bound $m=\Omega\bbr{\frac{\log(1/\delta)}{\log\log(1/\delta)}}$:
\begin{enumerate}
    \item The problem is further reduced to a testing problem \ref{hard_psd_testing}.
    The reduction is naturally to consider trace estimation of a random matrix $B$ generated from $P$ or $Q$ and find a gap between both confidence intervals. Actually, by concentration theorem \ref{rmt_smax_concen} we have $W + 6\sqrt{\log(1/\delta)}I_n$ is PSD $w.h.p.$, and $|tr(W)|\le 2\sqrt{2}\log(1/\delta)$ since $tr(W)\sim\mc N(0,4\log(1/\delta))$. Then, for any non-adaptive trace estimation algorithm with precision $\epsilon$, consider constant $C>\frac{10(1+\epsilon)}{1-\epsilon}-6$, then for trace estimation $t$, we have
    \begin{itemize}
        \item if $B\sim P$, then $t\ge(1-\epsilon)tr(B)=(1-\epsilon)((C+6)\log^{3/2}(1/\delta)-2\sqrt{2}\log(1/\delta))>6(1+\epsilon)\log^{3/2}(1/\delta)$ with probability at least $1-2\delta$,
        \item if $B\sim Q$, then $t\le(1+\epsilon)tr(B)\le(1+\epsilon)(6\log^{3/2}(1/\delta)+2\sqrt{2}\log(1/\delta))<6(1+\epsilon)\log^{3/2}(1/\delta)$ with probability at least $1-2\delta$,
    \end{itemize}
    which distinguishes $P$ and $Q$.
    
    \item Use hardness theorem \ref{hard_psd_testing_bound} we require at least $m=\Omega\bbr{\frac{\log(1/\delta)}{\log\log(1/\delta)}}$ queries. 
\end{enumerate}

\noindent Finally, combining these two bound by simple summation, we have our final lower bound.
}


\begin{rmk}
    Add spiked component $C\log^{3/2}(1/\delta)$ in the testing problem may be a standardized trick to analyze information-theoretic lower bound. Tuning the magnitude of the spiked component, we can show the phase transition by information-theoretic toolkit.
\end{rmk}

In this problem, the \textbf{gap between informational and computational threshold} is small(nearly matching), roughly an $O\bbr{\log\log(1/\delta)}$ constant.
The intuition is to improve Hutchinston's estiator by sketching a low rank approximation of $A$ and estimating the remaining component by Hutchinson’s estiator, rather than estimate $A$ itself. 
This induces a trade-off between sketching precision and Hutchinson’s estimation, optimizing the trade-off can reduce the variance.

\bigskip
\begin{breakablealgorithm}
\caption{Hutch++: randomized trace estimation with \textbf{adaptive} matrix-vector queries}
\label{hutch}
\begin{algorithmic}[1]
\Require{Matrix-vector product oracle for PSD matrix $A\in\rn n$. Number $m$ of queries.}
\Ensure{Approximation to $tr(A)$.}
\State Sample $S\in\rnm n{m/3},G\in\rnm n{m/3}$ with $i.i.d.$ $\mc N(0,1)$ entries.
\State Compute an orthonormal basis $Q\in\rnm n{m/3}$ for the span of $AS$ via $QR$ decomposition.
\State \textbf{return} $t=tr(AQQ^T)+\frac{3}{m}tr(G^TA(I-QQ^T)G)$.
\end{algorithmic}
\end{breakablealgorithm}
\bigskip

\begin{rmk}
Here, we take the randomness of Gaussian\cite{jpwz21}, while the original verision is rademacher's $\pm1$ variable\cite{mmmw21}. Note that,
Hutchinson's analysis works for general mean zero sub-Gaussian random variables.
\end{rmk}

\begin{thm}[Upper bound of adaptive queries\cite{mmmw21}]
\label{tr_est_ada_alg}
Let $m=O\bbr{\frac{\sqrt{\log(1/\delta)}}{\epsilon}+\log(1/\delta)}$. Then with probability at least $1-\delta$, Hutch++ yields an estimation $t$ such that $(1-\epsilon)tr(A)\le t \le (1+\epsilon)tr(A)$.
\end{thm}
\pfsk{\ref{tr_est_ada_alg}}{
First, note that $t=tr(AQQ^T)+H_{m/3}(A(I-QQ^T))$, where $H_l(\cdot)$ is Hutchinson’s Estimator(definition \ref{hutchinson_est}). 
Denote $\Delta:=A(I-QQ^T)$ and $\tilde{A}=AQQ^T$.
Thus, if $m=O\bbr{\log(1/\delta)}$, by theorem \ref{hutchinson_bound} we have  $|t-tr(A)|=|tr(\Delta)-H_{m/3}(\Delta)|\lesssim \sqrt{\frac{\log(1/\delta)}{m/3}}\|\Delta\|_F$ with probability(of $G$) at least $1-\delta$.

Then, if $m=O\bbr{k+\sqrt{\log(1/\delta)}}$, by theroem \ref{proj_preserve_sk} we have,
$AS/\sqrt{m}$ is a  $(1/9, 0, k)$-projection-cost-preserving of $A$(definition \ref{proj_preserve_def})
with probability(of $S$) at least $1-\delta$, and by theorem \ref{proj_preserve_def},  since $\|AS-ASQ_kQ_k^T\|= \|AS-(AS)_k\|_F$, we have $\|\Delta\|_F\le\|A-AQ_kQ_k^T\|_F\le\sqrt{\frac{1+1/9}{1-1/9}}\|A-A_k\|_F\le\sqrt{2}\|A-A_k\|_F$
and thus $\|\Delta\|_F\le\sqrt{2}\|A-A_k\|_F$ with probability(of $S$) at least $1-\delta$.

Therefore, if $k=\frac{\sqrt{\log(1/\delta)}}{\epsilon}$ and $m=O\bbr{\frac{\sqrt{\log(1/\delta)}}{\epsilon}+\log(1/\delta)}$, use lemma \ref{lra_tr_bound}, we have $|t-tr(A)|\lesssim\sqrt{\frac{\log(1/\delta)}{mk}}tr(A)\lesssim\epsilon\cdot tr(A)$ $w.h.p.$. 
}


\bigskip
\begin{breakablealgorithm}
\caption{NA-Hutch++: randomized trace estimation with \textbf{non-adaptive} matrix-vector queries}
\label{na_hutch}
\begin{algorithmic}[1]
\Require{Matrix-vector product oracle for PSD matrix $A\in\rn n$. Number $m$ of queries.}
\Ensure{Approximation to $tr(A)$.}
\State Fix constants $c_1,c_2,c_3$ such that $c_1<c_2$ and $c_1+c_2+c_3=1$.
\State Sample $S\in\rnm n{c_1m},R\in\rnm n{c_2m},G\in\rnm n{c_3m}$ with $i.i.d.$ $\mc N(0,1)$ entries.
\State \textbf{return} $t=tr(AR(S^TAR)^+S^TA)+\frac{1}{c_3m}tr(G^T(A-[AR(S^TAR)^+S^TA])G)$\footnote{Here $M^+$ means Moore-Penrose pseudoinverse of $M$}.
\end{algorithmic}
\end{breakablealgorithm}
\bigskip

\begin{thm}[Upper bound of non-adaptive queries\cite{jpwz21}]
\label{tr_est_nada_alg}
Let $m=O\bbr{\frac{\sqrt{\log(1/\delta)}}{\epsilon}+\log(1/\delta)}$. Then with probability at least $1-\delta$, NA-Hutch++ yields an estimation $t$ such that $(1-\epsilon)tr(A)\le t \le (1+\epsilon)tr(A)$.
\end{thm}
\pfsk{\ref{tr_est_nada_alg}}{
Similar to the adaptive one, here we divide $A$ into $\tilde{A}:=AR(S^TAR)^+S^TA$ and $\Delta:=A-\td A$. Based on the proof of theorem \ref{tr_est_ada_alg},
it suffices to show $\|\Delta\|_F=O(1)\|A-A_k\|_F$ $w.h.p.$ and then we can obtain exact the same bound as before. Formally, we will prove
if $m=O(k+\log(1/\delta))$, then with probability at least $1-\delta$ we have $\|\Delta\|_F=O(1)\|A-A_k\|_F$. 

Basic ideal is to use theorem \ref{regress_sketch} to do column and row sketching for regression respectively.

First, note that $\wtd X:= (S^TAR)^+S^TA=\arg\min_X\fnorm{S^T(ARX-A)}$ by theorem \ref{proj}, thus 
if $c_1 m=O(c_2 m+\log(1/\delta))$
we have $\|A-\wtd A\|_F\le O(1)\fnorm{A-ARX^*}$, where
$X^*=\arg\min_X\fnorm{ARX-A}$ with probability(of $S$) at least $1-\delta$.

Then, by definition we have $\fnorm{A-ARX^*}\le\fnorm{A-AR(A_kR)^+A_k}=\fnorm{A-\bar{X}A_k}$. Note that $\bar{X}:=\arg\min_X\fnorm{(XA_k-A)R}=AR(A_kR)^+$, 
thus if $c_2 m=O(k+\log(1/\delta))$
we have $\fnorm{A-\bar{X}A_k}\le O(1)\fnorm{A-AA_k^+A_k}=O(1)\fnorm{A-A_k}$ with probability(of $R$) at least $1-\delta$.

Finally, if $m=O(k+\log(1/\delta))$ we have $\fnorm{A-\wtd A}\le O(1)\fnorm{A-ARX^*}\le O(1)\fnorm{A-\bar{X}A_k}\le O(1)\fnorm{A-A_k}$ $w.h.p.$
}
\begin{rmk}
    \xb{compare the improvement with \cite{mmmw21}} for non-adaptive queries
\end{rmk}

\xb{even though the bound is tight for Gaussian, what about other randomness, for example rademacher?}

%---------------------------------------------------
\subsection{Eigenvalue estimation}
\cite{braverman2021gradient}
